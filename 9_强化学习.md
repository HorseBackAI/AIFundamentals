[//]: # (Image References)

[image1]: ./Images/DeepTraffic.png
[image2]: ./Images/Quarotor.png
[image3]: ./Images/AlphaGo.png
[image4]: ./Images/RL01.png
[image5]: ./Images/Drone01.png
[image6]: ./Images/Drone02.png
[image7]: ./Images/StateValue.png

# 第九章：强化学习

## Application #1

[Deep Learning for Self-Driving Cars](https://selfdrivingcars.mit.edu/deeptraffic/)

![alt text][image1]

- How does the car learn to do that? Self-driving.
- How do we, human, learn that? Driving.

## Key elements of the learning

- What does "Reinforcement" mean? 
- What lures us to learn?

[![alt text][image4]](https://en.wikipedia.org/wiki/Reinforcement_learning)

Key elements:

1. Agent
2. Environment
3. Action: agent takes actions
4. Observe a State
5. Get Reward / Penalty(negative reward)

## Applications (cont'd)

A drone learns to take off and then navigate (fly) to a designated position in the air.

What are the elements? 

- **State**

![alt text][image5]

![alt text][image6]

- **Action**

![alt text][image2]

Alpha Go and then Alpha Zero

![alt text][image3]

- What is the **Reward**? 
- When do we get the Reward?

- **State**: 棋盘，战局
- What do we learn? 
	+ In some a state, what **Action** I should take.
	+ **Policy**: a mapping from State to Action
	+ P(a|s): a probability of taking an action in a state
	+ Policy chosen by **Value(State)**

![alt text][image7]


## Rationale

强化学习：一种高效的探索方法，而探索就是从一个状态开始（仿真）走

RL: Exploration

走到底或者哪怕只走出一步，便会触到一个预期的值（奖励或惩罚）

Expected values

然后用公式推算每一个状态下的预期值

Backup these values to states V(s)

这样的推算过程会逐步收敛，即预期值不会来回乱变

Such backup computation converges

依据这些值，决定在每一个状态应该往哪儿走：也就是策略

According to these values, compute which action should be taken: the policy: P(s) -> a

**强化学习简洁美妙之处**

- 会假设在未知的时空深处，有一个奖励值在等待被探索
- 同时，一旦验证某路不通，会果断放弃，探索他途