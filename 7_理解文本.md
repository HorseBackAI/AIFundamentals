[//]: # (Image References)

[image1]: ./Images/SearchEngine01.png
[image2]: ./Images/SearchEngine02.png
[image3]: ./Images/DeepLearningWiki01.png
[image4]: ./Images/DeepLearningWiki02.png
[image5]: ./Images/BagOfWords.jpg
[image6]: ./Images/z1m1_SearchResult.png
[image7]: ./Images/tf-idf.png

# 第七章：理解文本

## To learn

- 搜索引擎的工作原理：文本特征：
	+ 词袋模型 Bag of words
	+ TF-IDF 词频-逆文档频率
- 理解文本主题：主题模型 Topic Model：Latent Semantic Analysis 潜在语义分析

## 1. Search Engine

英文搜索：deep learing

![alt text][image1]

中文搜索：深度学习

![alt text][image2]

搜索引擎是如何帮我们找到相关内容（网页）的？How can search engine help us find relevant content (web pages)?

Note: there is no labels for docs about its topics or content.

[英文Wiki](https://en.wikipedia.org/wiki/Deep_learning)

![alt text][image3]

![alt text][image4]

内容是文本：理解文本：通过文本的特征：表征文本的方法
Content is text (document): Understanding the document: 
Document features: Represent a document by its features

## 2. Bag of Words

一个文档就是一袋子词：计算每个词出现的次数：每个文档因此形成一个向量

A doc is a bag of words: Count the number of each word in the doc, **Term Frequency**: The counting numbers (**TF**) form a vector, which represents the doc.

一个简单有效的模型，忽略：

A simple but effective model, which ignores:

- 词语顺序 Word sequence
- 句子结构 Sentence structure

示例 Example: Book **P.124-125**.

![alt text][image5]

概念：

- **语料库 Corpus**：所有文档的集合 Collection of all docs
- **词典 Vocabulary**

## 3. TF-IDF

Term Frequency - Inverse Document Frequency

**词频**可以表征一个文档是如何由一组不同数量的词构成的。TF represents how a doc is formed by a group (bag) of words with different frequencies.

一个词在一篇文档中词频越高，这个词对于这篇文档的重要性越大。The higher one word's (term) frequency, the more important this word to this doc.

但是，**有的词在每篇文档中都大量出现，这个词对于一篇文档的重要性就不高了。这个词太常见了，特征不足**。However, if some words happen many times in all docs, their importance to each doc is low. Too usual, lack of differentiating (representational) features.

**每个词的IDF** = 总文章数 / 出现这个词的文章数

IDF of each word = Total number of docs in corpus / Number of docs that have the word in 

举例：搜索“z1m1”

![alt text][image6]

假设总文章数位100亿，出现“z1m1”的文章数是5，那么每个“z1m1”的IDF值就是：10^10 / 5。**所以可见：仅在少数文档中出现的词的IDF值大**。Assume that the total doc number of corpus is 10 billion and 'z1m1' happens in 5 docs. So, IDF of 'z1m1' is 10^10 / 5. **So, words that appear in only a few docs have large IDF values**.

一些**词**对于一个文档**越独特**，**这些词越能表征这篇文档**。It highlights the words that are more unique to a document, and thus better for characterizing it. 

为了避免IDF计算中出现除以零（一个词没在语料文章中出现过）的情况，以及避免计算结果太大或太小以至于难以比较，做处理：**IDF = log (总文章数 / (出现这个词的文章数 + 1))**。To avoid division by 0 where no doc has some a word, and to scale the output to a comparable range, update the calculation: **IDF = log (Total number of docs in corpus / (Number of docs that have the word in + 1))**

这样地

1. 每个词的 TF 和 IDF 相乘得到它的：TF-IDF = TF * IDF
2. 一个文档的所有词（除去停止词）的TF-IDF构成这个文档的特征向量 Words' TF-IDFs form a feature vector of the doc.

概念：

- 停止词 Stop words：例如助词“这个”，出现很多，但不能表征文档，即不适合做一个文档的特征
- 中文分词 Word segmentation

**Search engine uses TF-IDFs of docs to find content which is relevant to our query key words.**

Example:

![alt text][image7]

## 下一次课，理解文本主题

