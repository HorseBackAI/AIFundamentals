[//]: # (Image References)

[image1]: ./Images/SearchEngine01.png
[image2]: ./Images/SearchEngine02.png
[image3]: ./Images/DeepLearningWiki01.png
[image4]: ./Images/DeepLearningWiki02.png
[image5]: ./Images/BagOfWords.jpg
[image6]: ./Images/z1m1_SearchResult.png
[image7]: ./Images/tf-idf.png
[image8]: ./Images/TopicModel01.jpg
[image9]: ./Images/TopicModel02.jpg
[image10]: ./Images/TopicModel03.jpg

# 第七章：理解文本

## To learn

- 搜索引擎的工作原理：文本特征 Represent a document：
	+ 词袋模型 Bag of words
	+ 词频-逆文档频率 TF-IDF 
- 理解文本主题 Understanding the topics of a document：
	+ 主题模型 Topic Model
	+ 潜在语义分析 Latent Semantic Analysis 
- 应用 Applications

## 1. Search Engine

英文搜索：deep learing

![alt text][image1]

中文搜索：深度学习

![alt text][image2]

搜索引擎是如何帮我们找到相关内容（网页）的？How can search engine help us find relevant content (web pages)?

Note: there is no labels for docs about its topics or content.

[英文Wiki](https://en.wikipedia.org/wiki/Deep_learning)

![alt text][image3]

![alt text][image4]

内容是文本：理解文本：通过文本的特征：表征文本的方法
Content is text (document): Understanding the document: 
Document features: Represent a document by its features

## 2. Bag of Words

一个文档就是一袋子词：计算每个词出现的次数：每个文档因此形成一个向量

A doc is a bag of words: Count the number of each word in the doc, **Term Frequency**: The counting numbers (**TF**) form a vector, which represents the doc.

一个简单有效的模型，忽略：

A simple but effective model, which ignores:

- 词语顺序 Word sequence
- 句子结构 Sentence structure

示例 Example: Book **P.124-125**.

![alt text][image5]

概念：

- **语料库 Corpus**：所有文档的集合 Collection of all docs
- **词典 Vocabulary**

## 3. TF-IDF

Term Frequency - Inverse Document Frequency

**词频**可以表征一个文档是如何由一组不同数量的词构成的。TF represents how a doc is formed by a group (bag) of words with different frequencies.

一个词在一篇文档中词频越高，这个词对于这篇文档的重要性越大。The higher one word's (term) frequency, the more important this word to this doc.

但是，**有的词在每篇文档中都大量出现，这个词对于一篇文档的重要性就不高了。这个词太常见了，特征不足**。However, if some words happen many times in all docs, their importance to each doc is low. Too usual, lack of differentiating (representational) features.

**每个词的IDF** = 总文章数 / 出现这个词的文章数

IDF of each word = Total number of docs in corpus / Number of docs that have the word in 

举例：搜索“z1m1”

![alt text][image6]

假设总文章数位100亿，出现“z1m1”的文章数是5，那么每个“z1m1”的IDF值就是：10^10 / 5。**所以可见：仅在少数文档中出现的词的IDF值大**。Assume that the total doc number of corpus is 10 billion and 'z1m1' happens in 5 docs. So, IDF of 'z1m1' is 10^10 / 5. **So, words that appear in only a few docs have large IDF values**.

一些**词**对于一个文档**越独特**，**这些词越能表征这篇文档**。It highlights the words that are more unique to a document, and thus better for characterizing it. 

为了避免IDF计算中出现除以零（一个词没在语料文章中出现过）的情况，以及避免计算结果太大或太小以至于难以比较，做处理：**IDF = log (总文章数 / (出现这个词的文章数 + 1))**。To avoid division by 0 where no doc has some a word, and to scale the output to a comparable range, update the calculation: **IDF = log (Total number of docs in corpus / (Number of docs that have the word in + 1))**

这样地

1. 每个词的 TF 和 IDF 相乘得到它的：TF-IDF = TF * IDF
2. 一个文档的所有词（除去停止词）的TF-IDF构成这个文档的特征向量 Words' TF-IDFs form a feature vector of the doc.

概念：

- 停止词 Stop words：例如助词“这个”，出现很多，但不能表征文档，即不适合做一个文档的特征
- 中文分词 Word segmentation

**Search engine uses TF-IDFs of docs to find content which is relevant to our query key words.**

Example:

![alt text][image7]

## 4. 理解文本主题 Understanding the topics of a document

- 如何表达一个主题？用数学
- 一篇文章是否只有一个主题？

### 4.1 主题模型 Topic Model

举例：教育主题 Education topic

教育**主题词频**：Term frequency of education topic：Topic TF

	词典中每个词的词频 = 所有教育主题文章中该词出现的次数 / 所有教育主题文章总词数

	TF of each word in Vocabulary = Count of this word in all education-topic documents / Total word count of all education-topic documents

单一主题词频 TF of one topic:

![alt text][image8]

**问题**：

- 单一主题的文章其实很少 Document with only one topic is rare.
- 即使有，也缺少标注 No labels for topics in corpus.

--- 如何解决？---

**解决方法：主题模型**

![alt text][image9]

一篇文章包含多个主题：**文章由几个不同的单一主题按一定比重组合而成**

- 每个主题由**主题词频**表征
- 几个主题词频按**主题比重**加权即得：**文档词频**

A doc may include a few topics, each of which is represented by topic term-frequency (topic TF). Such topic TFs combine with their topic weights into document TF.

### 4.2 潜在语义分析 Latent Semantic Analysis

D = WT

- D 文章词频：就是我们熟悉的TF(TF-IDF)，已知，可以从语料表统计出来 TFs (or TF-IDF) of docs: Can be counted from corpus
- W 主题比重：未知，这就是我们想要的结果 Weights of topics: Unknown, the outcome we want
- T 主题词频：未知 TFs of Topics: Unknown

![alt text][image10]

通过非负矩阵分解，从 D 得到 W 和 T

Compute W and T from D by non-negative matrix factorization

## 5. 主题模型的应用 Application of topic model

基于主题的文本搜索和个性化推荐 Searching and individual recommendation based on topic model

解决基于关键词搜索的缺点 Mitigating problems of searching by key words：

- 忽略相近词 Ignore similar words
- 不同主题下一词多义 Polysemy in different topics















