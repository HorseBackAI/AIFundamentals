[//]: # (Image References)

[image1]: ./Images/Iris_setosa.jpg
[image2]: ./Images/Iris_versicolor.jpg
[image3]: ./Images/Iris_virginica.jpg
[image4]: ./Images/Iris_cluster_groud_truth.png
[image5]: ./Images/Clustering_marketing.png
[image6]: ./Images/Clustering_insurance.png
[image7]: ./Images/Iris_clustering.png
[image8]: ./Images/face_clustering.png
[image9]: ./Images/face_frontalization.png
[image10]: ./Images/face_encoding.jpg
[image11]: ./Images/face_recognition.png
[image12]: ./Images/face_detection.png
[image13]: ./Images/ElbowMethod.jpg[image14]: ./Images/HierarchicalClustering01.jpg[image15]: ./Images/HierarchicalClustering02.jpg

# 第六章：聚类 Clustering

“无师自通”：**Unsupervised learning**

没有事先标注好的类别了 No target labels any more

We have to find some patterns in the data points to cluster (note: not use the verb 'classify' here) data points into different classes. 

Clustering.

## To learn

- 问题：What if AI doesn't know the breeds (classes) of irises? How can it classify them?

---

- K-means clustering
	* Algorithm: 
		+ [Illustration](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
		+ 1) Initialize centroids --> 2) assign samples --> 3) re-vote centroids
	* Issues with K-means
	* [Example code: Irises clustering](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html)

---

- Human face clustering
	* Pipeline: 1) face detection --> 2) [face frontalization](https://github.com/dougsouza/face-frontalization) --> 3) feature extraction: face encoding --> 4) face clustering
	* [Face recognition example code](https://github.com/ageitgey/face_recognition): Key point here is actually about face encoding (feature extraction) by deep learning.

---

- Elbow method: how to choose a good K

---

- Hierarchical clustering and its application on biology
## 1. Applications of Clustering

**Find an intrinsic grouping in set of unlabeled examples.**

Of great practical utility:

- Marketing

![alt text][image5]

- Biology and medicine
- Insurance

![alt text][image6]

Like deep learning 下山 down the hill, it’s an **Optimization Problem**.

Need an objective:

- Low intra-cluster dissimilarity 组内类似
- High inter-cluster dissimilarity 组间不像


## 2. Any Idea?

Iris setosa

![alt text][image1]

Iris versicolor

![alt text][image2]

Iris virginica

![alt text][image3]

![alt text][image4]

## 3. K-means Clustering

### Algorithm

[Algorithm and example from lecture note of MIT 6.002](./6_2_Kmeans_mit.pdf)

1. Initialize centroids
2. (Re-)assign samples to closest centroid
3. Re-vote centroids by averaging samples

Repeat 2 thru 3 till centroids don't change

[Another illustration](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)

### Issues with K-means

Bad initial centroids and wrong K would lead to nonsense.

### Example code: [Irises clustering](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html)
	
	# No target classes
	X = iris.data
	...
	
	# 尝试做3次聚类，分别：
	# - 聚8个类，随机初始中心（默认）10次: 8 clusters, initialize centroids for 10 times and then select the best clustering result
	# - 聚3个类，随机初始中心（默认）10次: 3 clusters, initialize 10 times ... 
	# - 聚3个类，只随机初始中心一次, 3 clusters, initialize only once 
	estimators = [('k_means_iris_8', KMeans(n_clusters=8)),
	              ('k_means_iris_3', KMeans(n_clusters=3)),
	              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1, init='random'))]
	              
	...
	
	# 对每一次聚类
	for name, est in estimators:
	    ...
	    est.fit(X)   # K-means clustering 聚类
	    labels = est.labels_   # Clustering results 结果标注
	    
	    ax.scatter(X[:, 3], X[:, 0], X[:, 2],
	               c=labels.astype(np.float), # Label data points using different colors
	               edgecolor='k') 

![alt text][image7]

## 4. Face Clustering

![alt text][image8]

### Pipeline

- Face detection
- [Face frontalization](https://github.com/dougsouza/face-frontalization): Find landmarks on face, and then use such landmarks to frontalize the face.

![alt text][image9]

- **Feature extraction: face encoding:** features learned (encoded) by deep learning.

![alt text][image10]

- Face clustering

### Face recognition [Example Code](https://github.com/ageitgey/face_recognition)

- `face_recognition`：识别人脸身份

![alt text][image11]

With encoded face (face features), we can compare and recognize persons, i.e. cluster faces.

	import face_recognition
		
	picture_of_me = face_recognition.load_image_file("me.jpg")
	my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]
		
	# my_face_encoding now contains a universal 'encoding' 
	# of my facial features that can be compared to 
	# any other picture of a face!
		
	unknown_picture = face_recognition.load_image_file("unknown.jpg")
	unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]
		
	# Now we can see the two face encodings are of 
	# the same person with `compare_faces`!
		
	results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)
		
	if results[0] == True:
		print("It's a picture of me!")
	else:
		print("It's not a picture of me!")


- `face_detection`：人脸在图像中的位置

![alt text][image12]

- `face_recognition.face_landmarks(image)`：人脸关键点（可以用来美图）


### Takeaway of this example

- Search any application what you may be interested in
- Many out-of-box features can be played around. You may get some gut feeling and then utilize them to do something. : )

## 5. Elbow Method

Problem: how to choose a good K?

Recall: Issues with K-means?

![alt text][image13]

- As the number of centroids K increases, average distance between centroids and their clutering points decreases. 
- However, once K reaches the Elbow point, the average distance decreases much slower.
- A balance point to choose: good K.

## 6. Hierarchical Clustering

![alt text][image14]

Algorithm:

1. Treat each sample (data point) as one cluster.
2. **Form a cluster by joining the 2 most similar (closest) clusters**.
3. Untill all distances between classes exceed a certain pre-set value. Done.

Ref: [Hierarchical Clustering with Python and Scikit-Learn](https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/): 

- Agglomerative clustering that involves the bottom-up approach.
- Dendrogram: 系统树图（表示亲缘关系的树状图解）
- Hierarchical Clustering via Scikit-Learn 

![alt text][image15]

In Biology: discover implicit relations between breeds.

## 7. Review

- Clustering: unsupervised learning: unlabeled data (no target labels)
- K-means clustering
	1. Initialize centroids
	2. Assign data points to the nearest centroids
	3. Re-compute centroids
	4. Repeat step 2 and 3 till no centroids change
- Elbow method to find good K value
- Hierarchical clustering and its application